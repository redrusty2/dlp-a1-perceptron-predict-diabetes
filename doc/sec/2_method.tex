\section{Method}
\label{sec:method}

\subsection{Prediction}

We have implemented a very simple version of the Single Layer Perceptron (SLP) algorithm.

A SLP is a binary classifier which is a supervised machine learning algorithm. It is a binary classifier that takes in a vector of input features $\boldsymbol{x}$ and outputs a single value $y$ of 1 or 0 indicating which class the features belong to. 

\begin{equation}
\boldsymbol{x} = [x_1, x_2, \ldots, x_n]
\label{eq:input_vector_x}
\end{equation}

There is a weight vector $\boldsymbol{w}$ which is the same length as the input vector $\boldsymbol{x}$, and is learned during the training phase.

\begin{equation}
  \boldsymbol{w} = [w_1, w_2, \ldots, w_n]
  \label{eq:weight_vector_w}
\end{equation}

The output $y$ is determined by the decision function. The decision function is used to classify the input features into one of the classes. We initally use a basic binary classification decision function which takes the dot product of the inputs $\boldsymbol{x}$ and the weights $\boldsymbol{x}$ then passes this through an activation function. We use the sign function as the activation function.

% Mention the bias term here.

\begin{equation}
  y = \text{sign}(\boldsymbol{x} \cdot \boldsymbol{w})
  \label{eq:decision_function}
\end{equation}


\subsection{Training}

Our initial implementation of the SLP training algorithm uses a feedforward method.

We need a way to update the weights.

Given a training set of n samples $\{(\boldsymbol{x}_i, y_i)\}^{n}_{i=1}$


First the weights are initialised to small random values. This is important because of the indicator function in the decision function will stop a weight from updating if it is 0. This is because 

Now for each sample in the training set we make a prediciton $y^* = \text{sign}(\boldsymbol{x}_i \cdot \boldsymbol{w})$. We then use the prediction and the correct value to calculate the loss. The loss for each sample is given by 

% $l_i = \max\{0, -y \langle \boldsymbol{x_i}, \boldsymbol{w} \rangle\}$.

\begin{equation}
  l_i = \max\{0, -y_i \langle \boldsymbol{x_i}, \boldsymbol{w} \rangle\}
  \label{eq:loss}
\end{equation}

The loss gives us a metric to tell us how far away the prediction is from the correct output. Or at least it shows the "direction" we need to move the weights to get closer to the correct output.

We need to choose how much we want to update the weights(why?). To do this we use a hyperparameter called the learning rate $\eta$. The learning rate is a small positive number that we use to increase of decrease how much the weights are updated. \cite{Marsland2015}

Now that we have the loss we can update the weights. Each weight can be updated using the following equation.

\begin{equation}
  w_{i}^* = w_i - \eta \cdot l_i \cdot \boldsymbol{x}_i
  \label{eq:weight_update}
\end{equation}

This process is rep

We are given a training set of features with corresponding labels. For each sample in the training set we use the features to predict/caclulate $y$. We then compare the prediction with the correct label. We use an error/loss function to compute the loss/error which will help us update the weights. The loss is a metric to tell us how far away the prediction is from the correct output. Or at least it shows the "direction" we need to move the weights to get closer to the correct output.  


Initially we split the dataset into a training set and a test set with a 80/20 split. The samples for each set are chosen randomly. 

% Balanced dataset?

% Other preprocessing steps?


\subsection{Evaluation}

We evaluate all variations of the model using the accuracy metric and the F1 score. Where $TP$ is the number of true positives, $TN$ is the number of true negatives, $FP$ is the number of false positives and $FN$ is the number of false negatives. \cite{Marsland2015}

\begin{equation}
  \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
  \label{eq:accuracy}
\end{equation}

\begin{equation}
  F_1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
  \label{eq:f1}
\end{equation} 

Where 

\begin{equation}
  \text{Precision} = \frac{TP}{TP + FP}
  \label{eq:precision}
\end{equation}

\begin{equation}
  \text{Recall} = \frac{TP}{TP + FN}
  \label{eq:recall}
\end{equation}

We will also use the precision-recall curve to compare model variations between each other. It will also show us how well the models are performing compared to a random classifier. A PR curve is used since it is more informative than the ROC curve when dealing with imbalanced datasets \cite{scikit_learn_precision_recall}. 


We will construct the curve by using the results from stratified 8-fold cross-validation. The PR curve for each fold will be plotted as well as the mean plot curve. The area under the curve is also displayed. $k=8$ was chosen since it divides the dataset eqaully. With 768 samples we get 96 samples in each fold. Since the dataset is not balanced we use select each fold randomly but while maintaining the class distribution between folds \cite{Brownlee2023, Krasnoshchek2024, Brownlee2020_imbalanced, Psteen2020}.

\subsection{Dataset}

% Should we use imputation instead?? https://www.geeksforgeeks.org/ml-handling-missing-values/
There are 9 samples in the dataset that only have 8 features instead of 9. Since there is only a small number of samples with missing features we removed them from our dataset.



\section{Experiments}

\subsection{Baseline}

The results of the baseline model were actually not too bad. The mean accuracy was 0.75 and the mean F1 score was 0.62. The mean area under the PR curve was 0.68. 

\subsection{Adding a bias term}

Adding a bias term to the model was the simplest next step. The bias term is added as an extra weight the the weights vector. Then an additional input is added to the input vector which is always 1. Doing it this way lets it work wit hthe current baseline implementation other than adding the extra weight and input values.

Adding a bias term in theory should let the model learn a better decision boundary as it lets the decision bounday move away from the origin.

The results of adding the bias term were worse than the baseline which was surpising initially. The mean accuracy was 0.54 and the F1 score was 0.6. The mean area under the PR curve was 0.69 which was slightly better than the baseline but not by much.

The recall was 0.99 and the precision was 0.43 suggesting that the model is overfitting to the positive class.

\subsection{Added L2 Regularisation}

Added L2 regularisation to attempt to reduce potential overfitting. But it did not help in this case. 


\subsection{Training for recall}

By accident we trained an incorrect model that only updated it's weights if the label was 1. This model performed ok, but when we corrected the implementation it performed worse. This showed that focussing on recall could be a good idea.
